{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "U34PJNG32NeD",
        "colab_type": "code",
        "outputId": "126e8a41-41ab-49d5-df5d-a6143549dae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1156
        }
      },
      "source": [
        "\n",
        "from google.colab import drive \n",
        " \n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!unzip \"/content/gdrive/My Drive/miscellaneous/data1-2.h5.zip\"\n",
        "\n",
        "!ls\n",
        "\n",
        "import numpy as np \n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def build_baseline(width, height, depth, classes):\n",
        "\n",
        "  # initializing sequential model\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  # input shape is always required in the first layer\n",
        "  inputShape = (height, width, depth)\n",
        "  \n",
        "  # define the first and only CONV => POOL layer\n",
        "  model.add(tf.keras.layers.Conv2D (32, (3, 3), padding=\"same\",input_shape=inputShape, activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  # softmax classifier to identify the 17 classes of Flowers\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(classes, activation='softmax'))\n",
        "\n",
        "  return model\n",
        "\n",
        "def build_arch_2(width, height, depth, classes):\n",
        "\n",
        "  # initializing sequential model\n",
        "  model = tf.keras.Sequential()\n",
        "  \n",
        "  # input shape is always required in the first layer\n",
        "  inputShape = (height, width, depth)\n",
        "  \n",
        "  # define the first CONV => POOL layer\n",
        "  model.add(tf.keras.layers.Conv2D (32, (3, 3), padding=\"same\",input_shape=inputShape, activation='relu'))  \n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  \n",
        "  # define the second CONV => POOL layer\n",
        "  model.add(tf.keras.layers.Conv2D(32, (3, 3), padding=\"same\",activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))  \n",
        "  \n",
        "  # define the third CONV => POOL layer\n",
        "  model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\",activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))  \n",
        "\n",
        "  # define the only FC => RELU layer\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(512,activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(rate=0.2))    \n",
        "  \n",
        "  # softmax classifier to identify the 17 classes of Flowers\n",
        "  model.add(tf.keras.layers.Dense(classes, activation='softmax'))\n",
        "\n",
        "  return model\n",
        "\n",
        "def build_arch_3(width, height, depth, classes):\n",
        "\n",
        "  # initializing sequential model\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  # input shape is always required in the first layer\n",
        "  inputShape = (height, width, depth)\n",
        "  \n",
        "  # define the first CONV => CONV => POOL layer\n",
        "  model.add(tf.keras.layers.Conv2D(32, (3, 3), padding=\"same\",input_shape=inputShape, activation='relu'))\n",
        "  model.add(tf.keras.layers.Conv2D(32, (3, 3), padding=\"same\",activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  \n",
        "  # define the second CONV => CONV => POOL layer\n",
        "  model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\",activation='relu'))\n",
        "  model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\",activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  \n",
        "  # define the only FC => RELU layer\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(512,activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(rate=0.2)) \n",
        "  \n",
        "  # softmax classifier to identify the 17 classes of Flowers\n",
        "  model.add(tf.keras.layers.Dense(classes, activation='softmax'))\n",
        " \n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def loadDataH5():\n",
        "  \n",
        "  with h5py.File('data1.h5','r') as hf:\n",
        "    trainX = np.array(hf.get('trainX')) \n",
        "    trainY = np.array(hf.get('trainY')) \n",
        "    valX = np.array(hf.get('valX')) \n",
        "    valY = np.array(hf.get('valY')) \n",
        "    print (trainX.shape,trainY.shape) \n",
        "    print (valX.shape,valY.shape)\n",
        "    \n",
        "    return trainX, trainY, valX, valY\n",
        "  \n",
        "def cnn_augmentation():\n",
        "  \n",
        "  # num of epochs for the CNN augmentation. It is higher than the normal as dataset with augmentation took longer to diverge\n",
        "  NUM_EPOCHS = 100\n",
        "\n",
        "  # load the training and testing data\n",
        "  trainX, trainY, testX, testY = loadDataH5()\n",
        "\n",
        "  # initialize the optimizer and model\n",
        "  print(\"Compiling model...\")\n",
        "  opt = tf.keras.optimizers.SGD(lr=0.01)\n",
        "\n",
        "  model = build_arch_2(width=128, height=128, depth=3, classes=17)\n",
        "\n",
        "  print (model.summary())\n",
        "\n",
        "  model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
        "\n",
        "  # data augmentation for training data\n",
        "  trainDataGenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "      rotation_range=15,\n",
        "      #width_shift_range=0.1,\n",
        "      #height_shift_range=0.1,\n",
        "      horizontal_flip=True,\n",
        "      )\n",
        "\n",
        "  train_generator = trainDataGenerator.flow(\n",
        "    trainX,\n",
        "    trainY,\n",
        "    batch_size=32)\n",
        "\n",
        "  # data augmentation for training data for testing data, which is empty as data for validation is already scaled and no changes are needed.\n",
        "  valDataGenerator = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "\n",
        "  validation_generator = valDataGenerator.flow(\n",
        "    testX,\n",
        "    testY,\n",
        "    batch_size=32)\n",
        "\n",
        "  H = model.fit_generator(train_generator,\n",
        "         steps_per_epoch= 1020 // 32,\n",
        "          epochs=NUM_EPOCHS,\n",
        "          validation_data=validation_generator,\n",
        "          validation_steps=340 // 32                       \n",
        "         )\n",
        "\n",
        "def cnn():\n",
        "  \n",
        "  NUM_EPOCHS = 60\n",
        "\n",
        "  # load the training and testing data\n",
        "  trainX, trainY, testX, testY = loadDataH5()\n",
        "\n",
        "  # initialize the optimizer and model\n",
        "  print(\"Compiling model...\")\n",
        "  opt = tf.keras.optimizers.SGD(lr=0.01)\n",
        "\n",
        "  #model = build_baseline(width=128, height=128, depth=3, classes=17)\n",
        "  model = build_arch_2(width=128, height=128, depth=3, classes=17)\n",
        "  #model = build_arch_3(width=128, height=128, depth=3, classes=17)\n",
        "\n",
        "  print (model.summary())\n",
        "\n",
        "  model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
        "  \n",
        "\n",
        "  # train the network\n",
        "  print(\"Training network...\")\n",
        "  H = model.fit(trainX, trainY, validation_data=(testX, testY),batch_size=32, epochs=NUM_EPOCHS)\n",
        "\n",
        "\n",
        "  \n",
        "# executing CNN  \n",
        "cnn_augmentation()\n",
        "\n",
        "# plot the training loss and accuracy\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, 60), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, 60), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, 60), H.history[\"acc\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, 60), H.history[\"val_acc\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Archive:  /content/gdrive/My Drive/miscellaneous/data1-2.h5.zip\n",
            "replace data1.h5? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "data1.h5  gdrive  sample_data\n",
            "(1020, 128, 128, 3) (1020,)\n",
            "(340, 128, 128, 3) (340,)\n",
            "Compiling model...\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_54 (Conv2D)           (None, 128, 128, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_54 (MaxPooling (None, 64, 64, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_55 (Conv2D)           (None, 64, 64, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_55 (MaxPooling (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_56 (Conv2D)           (None, 32, 32, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_56 (MaxPooling (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_18 (Flatten)         (None, 16384)             0         \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 512)               8389120   \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 17)                8721      \n",
            "=================================================================\n",
            "Total params: 8,426,481\n",
            "Trainable params: 8,426,481\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "11/11 [==============================] - 0s 29ms/step - loss: 2.7956 - acc: 0.1147\n",
            "32/32 [==============================] - 3s 86ms/step - loss: 2.8250 - acc: 0.0863 - val_loss: 2.7956 - val_acc: 0.1147\n",
            "Epoch 2/100\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 2.7055 - acc: 0.2118\n",
            "32/32 [==============================] - 3s 94ms/step - loss: 2.7710 - acc: 0.1480 - val_loss: 2.7055 - val_acc: 0.2118\n",
            "Epoch 3/100\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 2.4440 - acc: 0.2000\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 2.6056 - acc: 0.1990 - val_loss: 2.4440 - val_acc: 0.2000\n",
            "Epoch 4/100\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 2.1683 - acc: 0.2618\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 2.3447 - acc: 0.2353 - val_loss: 2.1683 - val_acc: 0.2618\n",
            "Epoch 5/100\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 2.0722 - acc: 0.3471\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 2.1659 - acc: 0.2578 - val_loss: 2.0722 - val_acc: 0.3471\n",
            "Epoch 6/100\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.9192 - acc: 0.3235\n",
            "32/32 [==============================] - 3s 95ms/step - loss: 2.0367 - acc: 0.3029 - val_loss: 1.9192 - val_acc: 0.3235\n",
            "Epoch 7/100\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.8125 - acc: 0.3971\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.9415 - acc: 0.3314 - val_loss: 1.8125 - val_acc: 0.3971\n",
            "Epoch 8/100\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.8132 - acc: 0.4176\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.8876 - acc: 0.3578 - val_loss: 1.8132 - val_acc: 0.4176\n",
            "Epoch 9/100\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 1.7455 - acc: 0.4059\n",
            "32/32 [==============================] - 3s 103ms/step - loss: 1.8325 - acc: 0.3647 - val_loss: 1.7455 - val_acc: 0.4059\n",
            "Epoch 10/100\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.6788 - acc: 0.4176\n",
            "32/32 [==============================] - 3s 96ms/step - loss: 1.7560 - acc: 0.3980 - val_loss: 1.6788 - val_acc: 0.4176\n",
            "Epoch 11/100\n",
            " 7/32 [=====>........................] - ETA: 2s - loss: 1.6562 - acc: 0.4241"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_5gj921_j4f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10353
        },
        "outputId": "30eb23c1-e3a5-4c71-9b99-1b51a65d37a5"
      },
      "source": [
        "# Ensemble - The Architecture 2 with data augmentation will be used\n",
        "# Only one model is being used. This will cause a lack of diversity and results will be very similar\n",
        "\n",
        "def model_holder(trainX, trainY):\n",
        "  \n",
        "  \n",
        "  opt = tf.keras.optimizers.SGD(lr=0.01)\n",
        "  model = build_arch_2(width=128, height=128, depth=3, classes=17)\n",
        "  \n",
        "  model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
        "  \n",
        "  # data augmentation for training data\n",
        "  trainDataGenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "      rotation_range=15,\n",
        "      width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True,\n",
        "      )\n",
        "\n",
        "  train_generator = trainDataGenerator.flow(\n",
        "    trainX,\n",
        "    trainY,\n",
        "    batch_size=32)  \n",
        "  \n",
        "  H = model.fit(train_generator, batch_size=32, epochs=NUM_EPOCHS)  \n",
        "  \n",
        "  return model\n",
        "\n",
        "# comparing predicted with true y to get accuracy\n",
        "def accuracy_score(predicted_y, true_y):\n",
        "  \n",
        "  predictions_correct = np.equal(predicted_y, true_y)\n",
        "  \n",
        "  # sum of correct / len of items\n",
        "  accuracy = np.sum(predictions_correct.astype(int)) / len(predictions_correct)\n",
        "  \n",
        "  \n",
        "  return accuracy\n",
        "\n",
        "# main function to gather accuracy for individual members and the overall ensemble\n",
        "def ensemble_accuracy(members_probs):\n",
        "  \n",
        "  base_members_accuracy = []\n",
        "  \n",
        "  for i, member_probs in enumerate(members_probs):\n",
        "    \n",
        "    base_pred_y = np.argmax(member_probs, axis=1)\n",
        "    accuracy = accuracy_score(base_pred_y, testY)\n",
        "    \n",
        "    print(\"Base learner\", i + 1, \"accuracy: \", accuracy) \n",
        "\n",
        "  members_probs = np.array(members_probs)\n",
        "  \n",
        "  # argmax against ensemble\n",
        "  ensemble_probs = np.sum(members_probs, axis=0)\n",
        "  ensemble_pred_y = np.argmax(ensemble_probs, axis=1)  \n",
        "  \n",
        "  # calculate ensemble accuracy\n",
        "  ensemble_accuracy = accuracy_score(ensemble_pred_y, testY)\n",
        "  \n",
        "  print(\"Final accuracy:\", ensemble_accuracy)\n",
        "  \n",
        "# load the training and testing data\n",
        "trainX, trainY, testX, testY = loadDataH5()  \n",
        "\n",
        "NUM_EPOCHS = 60  \n",
        "base_learners = 5\n",
        "\n",
        "# create members model placeholder\n",
        "members = []\n",
        "\n",
        "for i in range(base_learners):\n",
        "  member = model_holder(trainX, trainY)\n",
        "  \n",
        "  # appending each model to a list of models\n",
        "  members.append(member)\n",
        "\n",
        "# Make predictions for each member\n",
        "members_probs = []\n",
        "\n",
        "for model in members:\n",
        "  \n",
        "  members_probs.append(model.predict(testX)) \n",
        "  \n",
        "  \n",
        "# call main function to print ensemble accuracy\n",
        "ensemble_accuracy(members_probs)    "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1020, 128, 128, 3) (1020,)\n",
            "(340, 128, 128, 3) (340,)\n",
            "Epoch 1/60\n",
            "32/32 [==============================] - 2s 71ms/step - loss: 2.8332 - acc: 0.0706\n",
            "Epoch 2/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 2.7995 - acc: 0.1059\n",
            "Epoch 3/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 2.7213 - acc: 0.1373\n",
            "Epoch 4/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 2.5245 - acc: 0.1951\n",
            "Epoch 5/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 2.3237 - acc: 0.2304\n",
            "Epoch 6/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 2.2058 - acc: 0.2667\n",
            "Epoch 7/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 2.1180 - acc: 0.2814\n",
            "Epoch 8/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.9975 - acc: 0.3088\n",
            "Epoch 9/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 1.9367 - acc: 0.3431\n",
            "Epoch 10/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.8895 - acc: 0.3490\n",
            "Epoch 11/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.8370 - acc: 0.3657\n",
            "Epoch 12/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 1.7809 - acc: 0.3892\n",
            "Epoch 13/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.7414 - acc: 0.3971\n",
            "Epoch 14/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.7116 - acc: 0.3941\n",
            "Epoch 15/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 1.7069 - acc: 0.4265\n",
            "Epoch 16/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.6524 - acc: 0.4275\n",
            "Epoch 17/60\n",
            "32/32 [==============================] - 3s 98ms/step - loss: 1.6206 - acc: 0.4343\n",
            "Epoch 18/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.5930 - acc: 0.4529\n",
            "Epoch 19/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.5186 - acc: 0.4804\n",
            "Epoch 20/60\n",
            "32/32 [==============================] - 3s 109ms/step - loss: 1.4845 - acc: 0.5010\n",
            "Epoch 21/60\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 1.4738 - acc: 0.4971\n",
            "Epoch 22/60\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 1.4389 - acc: 0.4971\n",
            "Epoch 23/60\n",
            "32/32 [==============================] - 3s 103ms/step - loss: 1.4268 - acc: 0.5353\n",
            "Epoch 24/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.3531 - acc: 0.5294\n",
            "Epoch 25/60\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 1.3369 - acc: 0.5275\n",
            "Epoch 26/60\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 1.2953 - acc: 0.5667\n",
            "Epoch 27/60\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 1.2843 - acc: 0.5520\n",
            "Epoch 28/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.2529 - acc: 0.5676\n",
            "Epoch 29/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.2143 - acc: 0.5824\n",
            "Epoch 30/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.1747 - acc: 0.6020\n",
            "Epoch 31/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.1803 - acc: 0.6000\n",
            "Epoch 32/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 1.1727 - acc: 0.5931\n",
            "Epoch 33/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.1336 - acc: 0.6039\n",
            "Epoch 34/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 1.1318 - acc: 0.6069\n",
            "Epoch 35/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.0506 - acc: 0.6304\n",
            "Epoch 36/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.0514 - acc: 0.6363\n",
            "Epoch 37/60\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 1.0233 - acc: 0.6588\n",
            "Epoch 38/60\n",
            "32/32 [==============================] - 3s 109ms/step - loss: 1.0403 - acc: 0.6598\n",
            "Epoch 39/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.9519 - acc: 0.6696\n",
            "Epoch 40/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.0331 - acc: 0.6441\n",
            "Epoch 41/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.9567 - acc: 0.6706\n",
            "Epoch 42/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.9412 - acc: 0.6627\n",
            "Epoch 43/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.8848 - acc: 0.6971\n",
            "Epoch 44/60\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.9206 - acc: 0.6882\n",
            "Epoch 45/60\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.9243 - acc: 0.6775\n",
            "Epoch 46/60\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.9236 - acc: 0.6873\n",
            "Epoch 47/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 0.8259 - acc: 0.7118\n",
            "Epoch 48/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.8294 - acc: 0.7069\n",
            "Epoch 49/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.8562 - acc: 0.7147\n",
            "Epoch 50/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.8148 - acc: 0.7069\n",
            "Epoch 51/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 0.8577 - acc: 0.7118\n",
            "Epoch 52/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.7598 - acc: 0.7451\n",
            "Epoch 53/60\n",
            "32/32 [==============================] - 3s 98ms/step - loss: 0.8016 - acc: 0.7147\n",
            "Epoch 54/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.7857 - acc: 0.7294\n",
            "Epoch 55/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.7949 - acc: 0.7167\n",
            "Epoch 56/60\n",
            "32/32 [==============================] - 3s 103ms/step - loss: 0.7938 - acc: 0.7167\n",
            "Epoch 57/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.7368 - acc: 0.7412\n",
            "Epoch 58/60\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.7588 - acc: 0.7451\n",
            "Epoch 59/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.7063 - acc: 0.7422\n",
            "Epoch 60/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.7375 - acc: 0.7480\n",
            "Epoch 1/60\n",
            "32/32 [==============================] - 2s 73ms/step - loss: 2.8245 - acc: 0.0971\n",
            "Epoch 2/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 2.7669 - acc: 0.1471\n",
            "Epoch 3/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 2.6281 - acc: 0.1725\n",
            "Epoch 4/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 2.4054 - acc: 0.1941\n",
            "Epoch 5/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 2.2576 - acc: 0.2480\n",
            "Epoch 6/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 2.1410 - acc: 0.2657\n",
            "Epoch 7/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 2.1078 - acc: 0.3000\n",
            "Epoch 8/60\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 1.9862 - acc: 0.3235\n",
            "Epoch 9/60\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 1.9399 - acc: 0.3441\n",
            "Epoch 10/60\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 1.8590 - acc: 0.3647\n",
            "Epoch 11/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.8599 - acc: 0.3569\n",
            "Epoch 12/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 1.7971 - acc: 0.3863\n",
            "Epoch 13/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.7671 - acc: 0.3931\n",
            "Epoch 14/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.7308 - acc: 0.4108\n",
            "Epoch 15/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.6494 - acc: 0.4314\n",
            "Epoch 16/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.6391 - acc: 0.4265\n",
            "Epoch 17/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.6493 - acc: 0.4402\n",
            "Epoch 18/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.6284 - acc: 0.4294\n",
            "Epoch 19/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.5468 - acc: 0.4745\n",
            "Epoch 20/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 1.5369 - acc: 0.4755\n",
            "Epoch 21/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.4792 - acc: 0.4922\n",
            "Epoch 22/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 1.5117 - acc: 0.4980\n",
            "Epoch 23/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 1.4362 - acc: 0.5020\n",
            "Epoch 24/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.3948 - acc: 0.5137\n",
            "Epoch 25/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.3925 - acc: 0.5088\n",
            "Epoch 26/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.3378 - acc: 0.5402\n",
            "Epoch 27/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.3261 - acc: 0.5441\n",
            "Epoch 28/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 1.2905 - acc: 0.5569\n",
            "Epoch 29/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.2826 - acc: 0.5461\n",
            "Epoch 30/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.2566 - acc: 0.5667\n",
            "Epoch 31/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.2039 - acc: 0.5843\n",
            "Epoch 32/60\n",
            "32/32 [==============================] - 3s 105ms/step - loss: 1.1733 - acc: 0.6039\n",
            "Epoch 33/60\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 1.1825 - acc: 0.6039\n",
            "Epoch 34/60\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 1.1590 - acc: 0.6049\n",
            "Epoch 35/60\n",
            "32/32 [==============================] - 4s 109ms/step - loss: 1.1296 - acc: 0.6069\n",
            "Epoch 36/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.0690 - acc: 0.6255\n",
            "Epoch 37/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.0906 - acc: 0.6343\n",
            "Epoch 38/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.0529 - acc: 0.6324\n",
            "Epoch 39/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.0826 - acc: 0.6275\n",
            "Epoch 40/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.0860 - acc: 0.6245\n",
            "Epoch 41/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.0387 - acc: 0.6324\n",
            "Epoch 42/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.0181 - acc: 0.6588\n",
            "Epoch 43/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 0.9946 - acc: 0.6480\n",
            "Epoch 44/60\n",
            "32/32 [==============================] - 3s 98ms/step - loss: 0.9499 - acc: 0.6735\n",
            "Epoch 45/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.8964 - acc: 0.6824\n",
            "Epoch 46/60\n",
            "32/32 [==============================] - 3s 98ms/step - loss: 0.9764 - acc: 0.6559\n",
            "Epoch 47/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.9067 - acc: 0.6775\n",
            "Epoch 48/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 0.8832 - acc: 0.6833\n",
            "Epoch 49/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.9055 - acc: 0.6971\n",
            "Epoch 50/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 0.9155 - acc: 0.6745\n",
            "Epoch 51/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.8606 - acc: 0.7029\n",
            "Epoch 52/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 0.8029 - acc: 0.7186\n",
            "Epoch 53/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 0.7829 - acc: 0.7255\n",
            "Epoch 54/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.8261 - acc: 0.7088\n",
            "Epoch 55/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 0.8193 - acc: 0.7127\n",
            "Epoch 56/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 0.7757 - acc: 0.7304\n",
            "Epoch 57/60\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 0.7659 - acc: 0.7294\n",
            "Epoch 58/60\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.7941 - acc: 0.7118\n",
            "Epoch 59/60\n",
            "32/32 [==============================] - 4s 122ms/step - loss: 0.7422 - acc: 0.7333\n",
            "Epoch 60/60\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.7445 - acc: 0.7500\n",
            "Epoch 1/60\n",
            "32/32 [==============================] - 2s 71ms/step - loss: 2.8233 - acc: 0.0843\n",
            "Epoch 2/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 2.7649 - acc: 0.1255\n",
            "Epoch 3/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 2.6222 - acc: 0.1784\n",
            "Epoch 4/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 2.4174 - acc: 0.2029\n",
            "Epoch 5/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 2.2796 - acc: 0.2294\n",
            "Epoch 6/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 2.1823 - acc: 0.2520\n",
            "Epoch 7/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 2.0578 - acc: 0.3137\n",
            "Epoch 8/60\n",
            "32/32 [==============================] - 3s 104ms/step - loss: 2.0201 - acc: 0.2980\n",
            "Epoch 9/60\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 1.9627 - acc: 0.3333\n",
            "Epoch 10/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 1.9205 - acc: 0.3441\n",
            "Epoch 11/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.8661 - acc: 0.3716\n",
            "Epoch 12/60\n",
            "32/32 [==============================] - 3s 104ms/step - loss: 1.8451 - acc: 0.3657\n",
            "Epoch 13/60\n",
            "32/32 [==============================] - 3s 98ms/step - loss: 1.7697 - acc: 0.4000\n",
            "Epoch 14/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.7360 - acc: 0.4098\n",
            "Epoch 15/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 1.6988 - acc: 0.4373\n",
            "Epoch 16/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.6952 - acc: 0.4324\n",
            "Epoch 17/60\n",
            "32/32 [==============================] - 3s 98ms/step - loss: 1.6371 - acc: 0.4343\n",
            "Epoch 18/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.5731 - acc: 0.4510\n",
            "Epoch 19/60\n",
            "32/32 [==============================] - 3s 98ms/step - loss: 1.5083 - acc: 0.4824\n",
            "Epoch 20/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.5344 - acc: 0.4735\n",
            "Epoch 21/60\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 1.5248 - acc: 0.4853\n",
            "Epoch 22/60\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 1.4142 - acc: 0.5039\n",
            "Epoch 23/60\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 1.3775 - acc: 0.5324\n",
            "Epoch 24/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.3802 - acc: 0.5333\n",
            "Epoch 25/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.3673 - acc: 0.5147\n",
            "Epoch 26/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.3430 - acc: 0.5559\n",
            "Epoch 27/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.2497 - acc: 0.5755\n",
            "Epoch 28/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.2869 - acc: 0.5569\n",
            "Epoch 29/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.2351 - acc: 0.5814\n",
            "Epoch 30/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.2257 - acc: 0.5882\n",
            "Epoch 31/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.2045 - acc: 0.5931\n",
            "Epoch 32/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.1410 - acc: 0.6108\n",
            "Epoch 33/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.1815 - acc: 0.5755\n",
            "Epoch 34/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 1.1227 - acc: 0.6020\n",
            "Epoch 35/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.0725 - acc: 0.6382\n",
            "Epoch 36/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 1.0865 - acc: 0.6176\n",
            "Epoch 37/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.0371 - acc: 0.6382\n",
            "Epoch 38/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.0396 - acc: 0.6373\n",
            "Epoch 39/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.0508 - acc: 0.6373\n",
            "Epoch 40/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.0281 - acc: 0.6588\n",
            "Epoch 41/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.9985 - acc: 0.6539\n",
            "Epoch 42/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.9999 - acc: 0.6412\n",
            "Epoch 43/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.9614 - acc: 0.6578\n",
            "Epoch 44/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.9329 - acc: 0.6598\n",
            "Epoch 45/60\n",
            "32/32 [==============================] - 3s 105ms/step - loss: 0.9346 - acc: 0.6686\n",
            "Epoch 46/60\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 0.9128 - acc: 0.6804\n",
            "Epoch 47/60\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.9029 - acc: 0.7098\n",
            "Epoch 48/60\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 0.8985 - acc: 0.6941\n",
            "Epoch 49/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.8848 - acc: 0.6922\n",
            "Epoch 50/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.9809 - acc: 0.6725\n",
            "Epoch 51/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 0.8552 - acc: 0.7049\n",
            "Epoch 52/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.8154 - acc: 0.7137\n",
            "Epoch 53/60\n",
            "32/32 [==============================] - 3s 103ms/step - loss: 0.7478 - acc: 0.7314\n",
            "Epoch 54/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 0.8367 - acc: 0.7127\n",
            "Epoch 55/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.7964 - acc: 0.7255\n",
            "Epoch 56/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.7308 - acc: 0.7373\n",
            "Epoch 57/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.7196 - acc: 0.7412\n",
            "Epoch 58/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.7058 - acc: 0.7716\n",
            "Epoch 59/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.6810 - acc: 0.7598\n",
            "Epoch 60/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 0.6757 - acc: 0.7549\n",
            "Epoch 1/60\n",
            "32/32 [==============================] - 2s 72ms/step - loss: 2.8138 - acc: 0.0745\n",
            "Epoch 2/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 2.7213 - acc: 0.1412\n",
            "Epoch 3/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 2.5281 - acc: 0.1853\n",
            "Epoch 4/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 2.3570 - acc: 0.2000\n",
            "Epoch 5/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 2.1933 - acc: 0.2431\n",
            "Epoch 6/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 2.1060 - acc: 0.2735\n",
            "Epoch 7/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 2.0386 - acc: 0.2843\n",
            "Epoch 8/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.9665 - acc: 0.3235\n",
            "Epoch 9/60\n",
            "32/32 [==============================] - 3s 107ms/step - loss: 1.9024 - acc: 0.3480\n",
            "Epoch 10/60\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 1.8121 - acc: 0.3804\n",
            "Epoch 11/60\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 1.8092 - acc: 0.3755\n",
            "Epoch 12/60\n",
            "32/32 [==============================] - 3s 106ms/step - loss: 1.7821 - acc: 0.3980\n",
            "Epoch 13/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.7126 - acc: 0.4176\n",
            "Epoch 14/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.6397 - acc: 0.4451\n",
            "Epoch 15/60\n",
            "32/32 [==============================] - 3s 103ms/step - loss: 1.6088 - acc: 0.4529\n",
            "Epoch 16/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.5732 - acc: 0.4794\n",
            "Epoch 17/60\n",
            "32/32 [==============================] - 3s 103ms/step - loss: 1.5405 - acc: 0.4706\n",
            "Epoch 18/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.5093 - acc: 0.5020\n",
            "Epoch 19/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.3971 - acc: 0.5235\n",
            "Epoch 20/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.4200 - acc: 0.5206\n",
            "Epoch 21/60\n",
            "32/32 [==============================] - 3s 98ms/step - loss: 1.3747 - acc: 0.5294\n",
            "Epoch 22/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.3533 - acc: 0.5402\n",
            "Epoch 23/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.3396 - acc: 0.5235\n",
            "Epoch 24/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.2518 - acc: 0.5784\n",
            "Epoch 25/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 1.2164 - acc: 0.5814\n",
            "Epoch 26/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.2472 - acc: 0.5657\n",
            "Epoch 27/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 1.1594 - acc: 0.5922\n",
            "Epoch 28/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.1665 - acc: 0.6127\n",
            "Epoch 29/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 1.1550 - acc: 0.5912\n",
            "Epoch 30/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.1524 - acc: 0.6029\n",
            "Epoch 31/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.0955 - acc: 0.6235\n",
            "Epoch 32/60\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 1.0924 - acc: 0.6324\n",
            "Epoch 33/60\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 1.0778 - acc: 0.6235\n",
            "Epoch 34/60\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 1.0613 - acc: 0.6275\n",
            "Epoch 35/60\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 1.0910 - acc: 0.6353\n",
            "Epoch 36/60\n",
            "32/32 [==============================] - 4s 109ms/step - loss: 1.0326 - acc: 0.6627\n",
            "Epoch 37/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.0110 - acc: 0.6608\n",
            "Epoch 38/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.0243 - acc: 0.6382\n",
            "Epoch 39/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.0049 - acc: 0.6539\n",
            "Epoch 40/60\n",
            "32/32 [==============================] - 3s 103ms/step - loss: 0.9344 - acc: 0.6716\n",
            "Epoch 41/60\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 0.8872 - acc: 0.6873\n",
            "Epoch 42/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.9065 - acc: 0.6863\n",
            "Epoch 43/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 0.8764 - acc: 0.7010\n",
            "Epoch 44/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.8920 - acc: 0.6745\n",
            "Epoch 45/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.8562 - acc: 0.7088\n",
            "Epoch 46/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.8467 - acc: 0.7069\n",
            "Epoch 47/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.8662 - acc: 0.7010\n",
            "Epoch 48/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.8049 - acc: 0.7284\n",
            "Epoch 49/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.8362 - acc: 0.7088\n",
            "Epoch 50/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.7878 - acc: 0.7304\n",
            "Epoch 51/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.7898 - acc: 0.7235\n",
            "Epoch 52/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.7578 - acc: 0.7304\n",
            "Epoch 53/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.7857 - acc: 0.7255\n",
            "Epoch 54/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.7146 - acc: 0.7608\n",
            "Epoch 55/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 0.7383 - acc: 0.7471\n",
            "Epoch 56/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 0.7017 - acc: 0.7578\n",
            "Epoch 57/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 0.6852 - acc: 0.7608\n",
            "Epoch 58/60\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.6683 - acc: 0.7559\n",
            "Epoch 59/60\n",
            "32/32 [==============================] - 4s 119ms/step - loss: 0.6557 - acc: 0.7706\n",
            "Epoch 60/60\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.6392 - acc: 0.7794\n",
            "Epoch 1/60\n",
            "32/32 [==============================] - 2s 71ms/step - loss: 2.8322 - acc: 0.0667\n",
            "Epoch 2/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 2.7907 - acc: 0.1137\n",
            "Epoch 3/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 2.6979 - acc: 0.1588\n",
            "Epoch 4/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 2.4804 - acc: 0.1980\n",
            "Epoch 5/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 2.3057 - acc: 0.2284\n",
            "Epoch 6/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 2.1934 - acc: 0.2500\n",
            "Epoch 7/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 2.0947 - acc: 0.2784\n",
            "Epoch 8/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 2.0275 - acc: 0.2941\n",
            "Epoch 9/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.9497 - acc: 0.3186\n",
            "Epoch 10/60\n",
            "32/32 [==============================] - 3s 98ms/step - loss: 1.8959 - acc: 0.3647\n",
            "Epoch 11/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.8745 - acc: 0.3637\n",
            "Epoch 12/60\n",
            "32/32 [==============================] - 3s 97ms/step - loss: 1.8385 - acc: 0.3696\n",
            "Epoch 13/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.7878 - acc: 0.4029\n",
            "Epoch 14/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.7660 - acc: 0.3784\n",
            "Epoch 15/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.7178 - acc: 0.4176\n",
            "Epoch 16/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.7018 - acc: 0.4098\n",
            "Epoch 17/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.7087 - acc: 0.3814\n",
            "Epoch 18/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.6247 - acc: 0.4451\n",
            "Epoch 19/60\n",
            "32/32 [==============================] - 3s 98ms/step - loss: 1.6569 - acc: 0.4333\n",
            "Epoch 20/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.6411 - acc: 0.4294\n",
            "Epoch 21/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.6064 - acc: 0.4441\n",
            "Epoch 22/60\n",
            "32/32 [==============================] - 3s 108ms/step - loss: 1.5617 - acc: 0.4471\n",
            "Epoch 23/60\n",
            "32/32 [==============================] - 4s 118ms/step - loss: 1.5633 - acc: 0.4539\n",
            "Epoch 24/60\n",
            "32/32 [==============================] - 4s 117ms/step - loss: 1.5459 - acc: 0.4637\n",
            "Epoch 25/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 1.5320 - acc: 0.4833\n",
            "Epoch 26/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.5003 - acc: 0.4578\n",
            "Epoch 27/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.5137 - acc: 0.4716\n",
            "Epoch 28/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.4032 - acc: 0.5039\n",
            "Epoch 29/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.4079 - acc: 0.5196\n",
            "Epoch 30/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 1.3989 - acc: 0.5088\n",
            "Epoch 31/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.3800 - acc: 0.5343\n",
            "Epoch 32/60\n",
            "32/32 [==============================] - 3s 98ms/step - loss: 1.3686 - acc: 0.5304\n",
            "Epoch 33/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.3378 - acc: 0.5382\n",
            "Epoch 34/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.2980 - acc: 0.5304\n",
            "Epoch 35/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.2661 - acc: 0.5676\n",
            "Epoch 36/60\n",
            "32/32 [==============================] - 3s 98ms/step - loss: 1.2023 - acc: 0.5882\n",
            "Epoch 37/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.2212 - acc: 0.5784\n",
            "Epoch 38/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.1928 - acc: 0.5686\n",
            "Epoch 39/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.1763 - acc: 0.5922\n",
            "Epoch 40/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.1177 - acc: 0.6049\n",
            "Epoch 41/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.1360 - acc: 0.6206\n",
            "Epoch 42/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.1140 - acc: 0.6157\n",
            "Epoch 43/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 1.0692 - acc: 0.6275\n",
            "Epoch 44/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.0552 - acc: 0.6284\n",
            "Epoch 45/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 1.0158 - acc: 0.6510\n",
            "Epoch 46/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 1.0083 - acc: 0.6471\n",
            "Epoch 47/60\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 1.0031 - acc: 0.6500\n",
            "Epoch 48/60\n",
            "32/32 [==============================] - 4s 120ms/step - loss: 0.9816 - acc: 0.6490\n",
            "Epoch 49/60\n",
            "32/32 [==============================] - 4s 116ms/step - loss: 0.9910 - acc: 0.6608\n",
            "Epoch 50/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.9754 - acc: 0.6627\n",
            "Epoch 51/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.9106 - acc: 0.6853\n",
            "Epoch 52/60\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.8995 - acc: 0.6902\n",
            "Epoch 53/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.9062 - acc: 0.6882\n",
            "Epoch 54/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 0.9207 - acc: 0.6765\n",
            "Epoch 55/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.8285 - acc: 0.7020\n",
            "Epoch 56/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 0.8468 - acc: 0.6931\n",
            "Epoch 57/60\n",
            "32/32 [==============================] - 3s 99ms/step - loss: 0.8561 - acc: 0.6961\n",
            "Epoch 58/60\n",
            "32/32 [==============================] - 3s 98ms/step - loss: 0.7904 - acc: 0.7382\n",
            "Epoch 59/60\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.8398 - acc: 0.7010\n",
            "Epoch 60/60\n",
            "32/32 [==============================] - 3s 102ms/step - loss: 0.7772 - acc: 0.7225\n",
            "Base learner 1 accuracy:  0.6852941176470588\n",
            "Base learner 2 accuracy:  0.6735294117647059\n",
            "Base learner 3 accuracy:  0.6647058823529411\n",
            "Base learner 4 accuracy:  0.6970588235294117\n",
            "Base learner 5 accuracy:  0.6441176470588236\n",
            "Final accuracy: 0.711764705882353\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN4pt3DbHBzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}