{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "U34PJNG32NeD",
        "colab_type": "code",
        "outputId": "00dabb9e-7291-44df-e28c-bd97c52b2527",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "\n",
        "from google.colab import drive \n",
        " \n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!unzip \"/content/gdrive/My Drive/miscellaneous/data1-2.h5.zip\"\n",
        "\n",
        "!ls\n",
        "\n",
        "import numpy as np \n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def build_baseline(width, height, depth, classes):\n",
        "\n",
        "  # initializing sequential model\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  # input shape is always required in the first layer\n",
        "  inputShape = (height, width, depth)\n",
        "  \n",
        "  # define the first and only CONV => POOL layer\n",
        "  model.add(tf.keras.layers.Conv2D (32, (3, 3), padding=\"same\",input_shape=inputShape, activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  # softmax classifier to identify the 17 classes of Flowers\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(classes, activation='softmax'))\n",
        "\n",
        "  return model\n",
        "\n",
        "def build_arch_2(width, height, depth, classes):\n",
        "\n",
        "  # initializing sequential model\n",
        "  model = tf.keras.Sequential()\n",
        "  \n",
        "  # input shape is always required in the first layer\n",
        "  inputShape = (height, width, depth)\n",
        "  \n",
        "  # define the first CONV => POOL layer\n",
        "  model.add(tf.keras.layers.Conv2D (32, (3, 3), padding=\"same\",input_shape=inputShape, activation='relu'))  \n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  \n",
        "  # define the second CONV => POOL layer\n",
        "  model.add(tf.keras.layers.Conv2D(32, (3, 3), padding=\"same\",activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))  \n",
        "  \n",
        "  # define the third CONV => POOL layer\n",
        "  model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\",activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))  \n",
        "\n",
        "  # define the only FC => RELU layer\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(512,activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(rate=0.2))    \n",
        "  \n",
        "  # softmax classifier to identify the 17 classes of Flowers\n",
        "  model.add(tf.keras.layers.Dense(classes, activation='softmax'))\n",
        "\n",
        "  return model\n",
        "\n",
        "def build_arch_3(width, height, depth, classes):\n",
        "\n",
        "  # initializing sequential model\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  # input shape is always required in the first layer\n",
        "  inputShape = (height, width, depth)\n",
        "  \n",
        "  # define the first CONV => CONV => POOL layer\n",
        "  model.add(tf.keras.layers.Conv2D(32, (3, 3), padding=\"same\",input_shape=inputShape, activation='relu'))\n",
        "  model.add(tf.keras.layers.Conv2D(32, (3, 3), padding=\"same\",activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  \n",
        "  # define the second CONV => CONV => POOL layer\n",
        "  model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\",activation='relu'))\n",
        "  model.add(tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\",activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  \n",
        "  # define the only FC => RELU layer\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(512,activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(rate=0.2)) \n",
        "  \n",
        "  # softmax classifier to identify the 17 classes of Flowers\n",
        "  model.add(tf.keras.layers.Dense(classes, activation='softmax'))\n",
        " \n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def loadDataH5():\n",
        "  \n",
        "  with h5py.File('data1.h5','r') as hf:\n",
        "    trainX = np.array(hf.get('trainX')) \n",
        "    trainY = np.array(hf.get('trainY')) \n",
        "    valX = np.array(hf.get('valX')) \n",
        "    valY = np.array(hf.get('valY')) \n",
        "    print (trainX.shape,trainY.shape) \n",
        "    print (valX.shape,valY.shape)\n",
        "    \n",
        "    return trainX, trainY, valX, valY\n",
        "  \n",
        "def cnn_augmentation():\n",
        "  \n",
        "  # num of epochs for the CNN augmentation. It is higher than the normal as dataset with augmentation took longer to diverge\n",
        "  NUM_EPOCHS = 100\n",
        "\n",
        "  # load the training and testing data\n",
        "  trainX, trainY, testX, testY = loadDataH5()\n",
        "\n",
        "  # initialize the optimizer and model\n",
        "  print(\"Compiling model...\")\n",
        "  opt = tf.keras.optimizers.SGD(lr=0.01)\n",
        "\n",
        "  model = build_arch_2(width=128, height=128, depth=3, classes=17)\n",
        "\n",
        "  print (model.summary())\n",
        "\n",
        "  model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
        "\n",
        "  # data augmentation for training data\n",
        "  trainDataGenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "      rotation_range=15,\n",
        "      width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      horizontal_flip=True,\n",
        "      )\n",
        "\n",
        "  train_generator = trainDataGenerator.flow(\n",
        "    trainX,\n",
        "    trainY,\n",
        "    batch_size=32)\n",
        "\n",
        "  # data augmentation for training data for testing data, which is empty as data for validation is already scaled and no changes are needed.\n",
        "  valDataGenerator = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "\n",
        "  validation_generator = valDataGenerator.flow(\n",
        "    testX,\n",
        "    testY,\n",
        "    batch_size=32)\n",
        "\n",
        "  H = model.fit_generator(train_generator,\n",
        "         steps_per_epoch= 1020 // 32,\n",
        "          epochs=NUM_EPOCHS,\n",
        "          validation_data=validation_generator,\n",
        "          validation_steps=340 // 32                       \n",
        "         )\n",
        "\n",
        "def cnn():\n",
        "  \n",
        "  NUM_EPOCHS = 60\n",
        "\n",
        "  # load the training and testing data\n",
        "  trainX, trainY, testX, testY = loadDataH5()\n",
        "\n",
        "  # initialize the optimizer and model\n",
        "  print(\"Compiling model...\")\n",
        "  opt = tf.keras.optimizers.SGD(lr=0.01)\n",
        "\n",
        "  #model = build_baseline(width=128, height=128, depth=3, classes=17)\n",
        "  model = build_arch_2(width=128, height=128, depth=3, classes=17)\n",
        "  #model = build_arch_3(width=128, height=128, depth=3, classes=17)\n",
        "\n",
        "  print (model.summary())\n",
        "\n",
        "  model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
        "  \n",
        "\n",
        "  # train the network\n",
        "  print(\"Training network...\")\n",
        "  H = model.fit(trainX, trainY, validation_data=(testX, testY),batch_size=32, epochs=NUM_EPOCHS)\n",
        "\n",
        "\n",
        "  \n",
        "# executing CNN  \n",
        "cnn_augmentation()\n",
        "\n",
        "# plot the training loss and accuracy\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, 60), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, 60), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, 60), H.history[\"acc\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, 60), H.history[\"val_acc\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6639c72b96ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#!unzip \"/content/gdrive/My Drive/miscellaneous/data1-2.h5.zip\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;34m': timeout during initial read of root folder; for more info: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             'https://research.google.com/colaboratory/faq.html#drive-timeout')\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Not already authorized, so do the authorization dance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    }
  ]
}